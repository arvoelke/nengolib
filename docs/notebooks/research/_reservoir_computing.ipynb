{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  },
  "name": "",
  "signature": "sha256:a87b0e88f14ee7caaeb249f9a57ebfc7d7e50736d17e6eee1b2def9ab2c565be"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Reservoir Computing in Nengo\n",
      "\n",
      "This notebook shows how reservoir computing can be formulated naturally within Nengo, with the additional benefit of having a principled way to build structure into the reservoir in both spiking (_Liquid State Machine_ (LSM)) and non-spiking (_Echo State Network_ (ESN)) cases. This allows us to understand how the reservoir is computing, improve simulation times by a factor $O(n)$, and improve performance on spiking hardware."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import pylab\n",
      "try:\n",
      "    import seaborn as sns  # optional; prettier graphs\n",
      "except ImportError:\n",
      "    pass\n",
      "\n",
      "import numpy as np\n",
      "import nengo\n",
      "from nengo.solvers import LstsqL2\n",
      "from nengo.utils.numpy import rmse, rms"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Structured Reservoirs\n",
      "\n",
      "We revisit the `PadeDelay` network from `doc/notebooks/examples/linear_network.ipynb`, which approximated a pure delay of $\\tau$ seconds using the transfer function $H(s) = e^{-\\tau s}$. This is implemented below using $1000$ `LIF` neurons and a `Lowpass` synapse. The `radii` were set by inspecting the absolute values of `subnet.state.input` in response to a test stimulus.\n",
      "\n",
      "We use this to delay an $8$ Hz band-limited white noise signal by $90$ ms. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nengolib\n",
      "from nengolib.networks import Reservoir, EchoState, LinearNetwork\n",
      "from nengolib.synapses import PadeDelay, DiscreteDelay\n",
      "\n",
      "n_neurons = 1000\n",
      "neuron_type = nengo.neurons.LIF()\n",
      "synapse = 0.005\n",
      "dt = 0.0005\n",
      "\n",
      "delay = 0.09\n",
      "function = lambda x: DiscreteDelay(int(delay/dt)).filt(x, dt=dt)\n",
      "\n",
      "train_t = 2.0\n",
      "test_t = 2.0\n",
      "process = nengo.processes.WhiteSignal(max(train_t, test_t), high=8, rms=0.3)\n",
      "\n",
      "with nengolib.Network() as model:\n",
      "    subnet = LinearNetwork(\n",
      "        PadeDelay(delay, order=5), n_neurons_per_ensemble=n_neurons, synapse=synapse,\n",
      "        radii=[0.1, 0.1, 0.16, 0.21, 0.35], dt=dt, neuron_type=neuron_type, seed=0,\n",
      "        solver=LstsqL2(reg=1e-2))\n",
      "    \n",
      "    stim = nengo.Node(output=process)\n",
      "    nengo.Connection(stim, subnet.input, synapse=None)\n",
      "    \n",
      "    output = nengo.Node(size_in=1)\n",
      "    nengo.Connection(subnet.output, output, synapse=synapse)\n",
      "    \n",
      "    p_stim = nengo.Probe(stim, synapse=None)\n",
      "    p_output = nengo.Probe(output, synapse=None)\n",
      "    \n",
      "freqs = np.linspace(0, 60, 100)\n",
      "pylab.figure()\n",
      "pylab.title(\"Frequency Response of Approximate Delay\")\n",
      "pylab.plot(freqs, np.ones_like(freqs), label=\"Ideal\")\n",
      "pylab.plot(freqs, abs(subnet.sys.evaluate(freqs)), label=\"Approximation\")\n",
      "pylab.vlines([process.high], 0, 1, label=\"Band-limit\", linestyle='--')\n",
      "pylab.ylim(0, 1.1)\n",
      "pylab.xlabel(\"Frequency ($Hz$)\")\n",
      "pylab.ylabel(\"Power\")\n",
      "pylab.legend(loc='center right')\n",
      "pylab.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This uses Principle 3 from the NEF to map the desired dynamics onto a recurrently connected ensemble of neurons. But instead of using the readout that was solved for using Principle 2, we can place the ensemble in a `Reservoir`, and solve for the optimal linear readout from the neurons in the linear network's `EnsembleArray` that represents the state vector."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "res = Reservoir(\n",
      "    inputs=subnet.input, outputs=subnet.state.add_neuron_output(),\n",
      "    network=subnet, readout_synapse=synapse)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We then train the reservoir on some realization of the random process (ignoring the first $180$ ms of data to allow the reservoir some time to initialize its state), which learns a linear readout to `res.output`. This new node is probed by simulating the network with a different random signal."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Training\n",
      "train_seed = 2\n",
      "res.train(function, train_t, dt, process,\n",
      "          t_init=2*delay, solver=LstsqL2(reg=0.1), seed=train_seed)\n",
      "\n",
      "with model:\n",
      "    p_refined = nengo.Probe(res.output, synapse=None)\n",
      "\n",
      "# Validation\n",
      "test_seed = 1\n",
      "with nengo.Simulator(model, dt=dt, seed=test_seed) as sim:\n",
      "    sim.run(test_t)\n",
      "\n",
      "def plot_results(t, y, y_est, title):\n",
      "    offset = int(2 * delay / dt)\n",
      "    error = rmse(y[offset:], y_est[offset:]) / rms(y[offset:])\n",
      "\n",
      "    pylab.figure()\n",
      "    pylab.title(\"%s (RMSE/RMS=%.4f)\" % (title, error))\n",
      "    pylab.plot(t[offset:], y_est[offset:], label=\"Actual\")\n",
      "    pylab.plot(t[offset:], y[offset:], label=\"Ideal\")\n",
      "    pylab.legend()\n",
      "    pylab.show()\n",
      "\n",
      "plot_results(sim.trange(), function(sim.data[p_stim]), sim.data[p_output], title=\"Structured\")\n",
      "#plot_results(info['sim'].trange(), function(info['data_in']), np.dot(info['data_mid'], d), title=\"Refined - Training\")\n",
      "plot_results(sim.trange(), function(sim.data[p_stim]), sim.data[p_refined], title=\"Refined\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first plot (structured) is the network built using standard Nengo / NEF theory, which does pretty well, but does not account for the temporal dynamics induced by the spiking `LIF` (note that changing the neuron type to `LIFRate` will give a nearly perfect match).\n",
      "\n",
      "The second plot (refined) accounts for the neural dynamics by solving for the linear readout from the reservoir (which contains the ensemble representing the state of the system). This does much better at aligning the signal in time, because it has refined the readout to include the dynamics of the neurons, while also compensating for any approximation error in the state vector (in this example the latter source of error is relatively negligible [see the very first figure], but it may be more considerable for other systems).\n",
      "\n",
      "This more holistic view of the optimization problem allows us to improve performance with the same amount of resources. We remark that these two networks are equivalent, except the second uses a _different_ linear readout from the neurons learned by explicitly simulating the network. \n",
      "\n",
      "### Random Reservoirs\n",
      "\n",
      "Now for the question: \"How does this compare to a random reservoir (i.e., ESN or LSM)\"? For this we use a standard `EchoState` network implemented in Nengo, which uses the same `Reservoir` class but also adds in a random pool of neurons. We use the same number of neurons, the same synapse, and the same test signal to keep comparisons fair."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_and_test(esn, reg, subtitle):\n",
      "    # Training\n",
      "    d, info = esn.train(\n",
      "        function, train_t, dt, process, t_init=2*delay, solver=LstsqL2(reg=reg), seed=train_seed)\n",
      "    #plot_results(info['sim'].trange(), function(info['data_in']),\n",
      "    #             np.dot(info['data_mid'], d), title=\"%s - Training\" % subtitle)\n",
      "    \n",
      "    # Validation\n",
      "    sim, (data_in, data_mid, data_out) = esn.run(test_t, dt, process, seed=test_seed)\n",
      "    assert np.allclose(np.dot(data_mid, d), data_out)\n",
      "    plot_results(sim.trange(), function(data_in), data_out,\n",
      "                 title=subtitle)\n",
      "\n",
      "\n",
      "rng = np.random.RandomState(0)\n",
      "\n",
      "with nengolib.Network():\n",
      "    esn = EchoState(n_neurons, 1, recurrent_synapse=synapse, readout_synapse=synapse, rng=rng)\n",
      "    train_and_test(esn, reg=1e-4, subtitle=\"Echo State Network\")\n",
      "    \n",
      "with nengolib.Network():\n",
      "    esn = EchoState(n_neurons, 1, neuron_type=nengo.LIF(), include_bias=False,\n",
      "                    recurrent_synapse=synapse, readout_synapse=synapse, gain=1.25/250,\n",
      "                    ens_seed=0, rng=rng)\n",
      "    train_and_test(esn, reg=0.1, subtitle=\"Liquid State Machine\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first plot (echo state) uses non-spiking `Tanh` neurons, with a standard set of parameters that have also been tuned for this example. It does slightly better than the spiking Nengo reservoir (both structured and refined), but this should not be surprising since it uses rate neurons. We can also tune them to be the same by using `Tanh` rate units for Nengo as well.\n",
      "\n",
      "The second plot (liquid state) uses spiking `LIF` neurons, and normalizes the recurrent gain by something like the average firing rate. This does fairly poorly compared to Nengo."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}