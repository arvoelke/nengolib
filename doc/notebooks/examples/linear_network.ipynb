{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Network\n",
    "\n",
    "The `nengolib.LinearNetwork` class is a `nengo` network that abstracts away many of the details of Principle 3 from the NEF. Simply supply a `LinearSystem` that you would like to simulate, and the network will map its dynamics onto a recurrently connected ensemble using a given `synapse`. This can be understood equivalently as convolving a causal filter with some arbitrary input signal. \n",
    "\n",
    "### Delay Example\n",
    "\n",
    "For example, the `nengolib.synapses.PadeDelay` is a linear system that we can build into a biologically plausible population of neurons to delay an input signal by some fixed amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pylab\n",
    "try:\n",
    "    import seaborn as sns  # optional; prettier graphs\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "import numpy as np\n",
    "import nengo\n",
    "import nengolib\n",
    "from nengolib.synapses import PadeDelay\n",
    "\n",
    "delay = 0.1\n",
    "T = 2.0\n",
    "dt = 0.001\n",
    "\n",
    "with nengolib.Network() as model:\n",
    "    stim = nengo.Node(output=nengo.processes.WhiteSignal(T, high=10))\n",
    "    \n",
    "    # Build a LinearNetwork that approximations a delay\n",
    "    subnet = nengolib.networks.LinearNetwork(\n",
    "        PadeDelay(3, 4, delay), n_neurons=200, synapse=0.02,\n",
    "        radii=1.5, dt=dt)\n",
    "    nengo.Connection(stim, subnet.input, synapse=None)\n",
    "\n",
    "    # Add some probes\n",
    "    p = nengo.Probe(subnet.output, synapse=0.01)\n",
    "    p_stim = nengo.Probe(stim, synapse=0.01)\n",
    "    \n",
    "sim = nengo.Simulator(model, dt=dt)\n",
    "sim.run(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize the difference between the ideal shifted input and the actual output of the network to see how accurately it performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offset = int(delay / dt + 1)\n",
    "\n",
    "pylab.figure()\n",
    "pylab.title('Delayed Input Signal')\n",
    "pylab.plot(sim.trange()[offset:], sim.data[p][offset:], label=\"Actual\")\n",
    "pylab.plot(sim.trange()[offset:], sim.data[p_stim][:-offset], label=\"Expected\")\n",
    "pylab.legend()\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Radii\n",
    "\n",
    "One of the most subtle but difficult aspects of building these networks is making sure that the representational range that the neurons are optimized over (the eval points and the radius in Nengo), matches the states that the network will actually visit.\n",
    "\n",
    "Crucially, this depends on how characteristics of the input signal relate to the given dynamics. By default, the `LinearNetwork` class plans for the worst-case input within the given radius, and accordingly plays safe by over-estimating the range of states that the neurons should represent.\n",
    "\n",
    "If we have more information about the problem, say by having looked at the responses of the states for typical inputs, we may use that to shrink the radii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    p_x = nengo.Probe(subnet.x.output, synapse=0.01)\n",
    "\n",
    "sim = nengo.Simulator(model, dt=dt)\n",
    "sim.run(T)\n",
    "\n",
    "pylab.figure()\n",
    "pylab.title('States')\n",
    "pylab.plot(sim.trange(), sim.data[p_x])\n",
    "pylab.xlabel('Time (s)')\n",
    "pylab.show()\n",
    "\n",
    "radii = np.max(abs(sim.data[p_x]), axis=0)\n",
    "print radii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on setting radii will be added here in the future, as well as simulating networks with other synapses on both analog and digital hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Input and Output Filtering\n",
    "\n",
    "By default the network will filter the input signal with the same synapse as the recurrent connection. This is how an LTI system is formulated, and so we do that here as well. Thus the _unfiltered_ output from the network is the desired system convolved with the input signal. \n",
    "\n",
    "However, if the input signal has already been filtered, and/or we would like the _filtered_ output to correspond to the desired system, then you should pass __`input_synapse=None`__ as an argument. This will avoid filtering the input, in effect giving an output that is a deconvolved version of the desired signal. Filtering this output with the same time-constant will then implement the desired system. But it is important to note that this only holds if the transfer function is strictly proper (it has a zero passthrough $D$ in state-space), otherwise the passthrough component will erroneously have a filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
